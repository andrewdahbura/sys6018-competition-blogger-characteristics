{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Blogger Age Prediction\n",
    "Team 2-9"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Data Exploration"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [],
   "source": [
    "import pandas as pd\n",
    "\n",
    "train = pd.read_csv(\"train.csv\")\n",
    "test = pd.read_csv(\"test.csv\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>post.id</th>\n",
       "      <th>user.id</th>\n",
       "      <th>gender</th>\n",
       "      <th>topic</th>\n",
       "      <th>sign</th>\n",
       "      <th>date</th>\n",
       "      <th>text</th>\n",
       "      <th>age</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>1</td>\n",
       "      <td>11869</td>\n",
       "      <td>male</td>\n",
       "      <td>Student</td>\n",
       "      <td>Leo</td>\n",
       "      <td>14,May,2004</td>\n",
       "      <td>Info has been found (+/- 100 pages,...</td>\n",
       "      <td>15</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1</th>\n",
       "      <td>2</td>\n",
       "      <td>11869</td>\n",
       "      <td>male</td>\n",
       "      <td>Student</td>\n",
       "      <td>Leo</td>\n",
       "      <td>13,May,2004</td>\n",
       "      <td>These are the team members:   Drewe...</td>\n",
       "      <td>15</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2</th>\n",
       "      <td>3</td>\n",
       "      <td>11869</td>\n",
       "      <td>male</td>\n",
       "      <td>Student</td>\n",
       "      <td>Leo</td>\n",
       "      <td>12,May,2004</td>\n",
       "      <td>In het kader van kernfusie op aarde...</td>\n",
       "      <td>15</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>3</th>\n",
       "      <td>4</td>\n",
       "      <td>11869</td>\n",
       "      <td>male</td>\n",
       "      <td>Student</td>\n",
       "      <td>Leo</td>\n",
       "      <td>12,May,2004</td>\n",
       "      <td>testing!!!  testing!!!</td>\n",
       "      <td>15</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>4</th>\n",
       "      <td>5</td>\n",
       "      <td>16332</td>\n",
       "      <td>male</td>\n",
       "      <td>InvestmentBanking</td>\n",
       "      <td>Aquarius</td>\n",
       "      <td>11,June,2004</td>\n",
       "      <td>Thanks to Yahoo!'s Toolbar I can ...</td>\n",
       "      <td>33</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "</div>"
      ],
      "text/plain": [
       "   post.id  user.id gender              topic      sign          date  \\\n",
       "0        1    11869   male            Student       Leo   14,May,2004   \n",
       "1        2    11869   male            Student       Leo   13,May,2004   \n",
       "2        3    11869   male            Student       Leo   12,May,2004   \n",
       "3        4    11869   male            Student       Leo   12,May,2004   \n",
       "4        5    16332   male  InvestmentBanking  Aquarius  11,June,2004   \n",
       "\n",
       "                                                text  age  \n",
       "0             Info has been found (+/- 100 pages,...   15  \n",
       "1             These are the team members:   Drewe...   15  \n",
       "2             In het kader van kernfusie op aarde...   15  \n",
       "3                   testing!!!  testing!!!             15  \n",
       "4               Thanks to Yahoo!'s Toolbar I can ...   33  "
      ]
     },
     "execution_count": 3,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "train.head()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [],
   "source": [
    "age = pd.Series(train['age'])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "image/png": "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\n",
      "text/plain": [
       "<Figure size 432x288 with 1 Axes>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    }
   ],
   "source": [
    "#histogram of ages\n",
    "plots = age.plot(kind = \"hist\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "female    227840\n",
       "male      215121\n",
       "Name: gender, dtype: int64"
      ]
     },
     "execution_count": 7,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "#count of posts by each gender\n",
    "gender = pd.Series(train['gender'])\n",
    "gender.value_counts()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "indUnk                     0.359894\n",
       "Student                    0.231953\n",
       "Technology                 0.063992\n",
       "Arts                       0.054887\n",
       "Education                  0.041907\n",
       "Communications-Media       0.029788\n",
       "Non-Profit                 0.020399\n",
       "Internet                   0.020013\n",
       "Engineering                0.016117\n",
       "Publishing                 0.012674\n",
       "Law                        0.012421\n",
       "Science                    0.011008\n",
       "Consulting                 0.010249\n",
       "Government                 0.008908\n",
       "Religion                   0.007825\n",
       "BusinessServices           0.007798\n",
       "Marketing                  0.007066\n",
       "Fashion                    0.006220\n",
       "Chemicals                  0.006050\n",
       "Museums-Libraries          0.005951\n",
       "Sports-Recreation          0.005696\n",
       "Advertising                0.005648\n",
       "Telecommunications         0.005563\n",
       "HumanResources             0.005134\n",
       "Banking                    0.004964\n",
       "Accounting                 0.004933\n",
       "Military                   0.004052\n",
       "RealEstate                 0.003775\n",
       "Transportation             0.003425\n",
       "Manufacturing              0.003411\n",
       "Tourism                    0.002905\n",
       "InvestmentBanking          0.002768\n",
       "Biotech                    0.002668\n",
       "Architecture               0.002486\n",
       "Agriculture                0.001876\n",
       "LawEnforcement-Security    0.001490\n",
       "Construction               0.001413\n",
       "Automotive                 0.001341\n",
       "Environment                0.000971\n",
       "Maritime                   0.000363\n",
       "Name: topic, dtype: float64"
      ]
     },
     "execution_count": 8,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "#how many posts about each topic\n",
    "topic = pd.Series(train['topic'])\n",
    "topics = topic.value_counts()\n",
    "topics/len(topic)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "<matplotlib.axes._subplots.AxesSubplot at 0x10a566b70>"
      ]
     },
     "execution_count": 9,
     "metadata": {},
     "output_type": "execute_result"
    },
    {
     "data": {
      "image/png": "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\n",
      "text/plain": [
       "<Figure size 432x288 with 1 Axes>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    }
   ],
   "source": [
    "#histogram for top age of indunk, mirrors general breakdown\n",
    "indunk = train.loc[train['topic'] == 'indUnk']\n",
    "indunkage = indunk['age']\n",
    "indunkage.plot(kind = 'hist')"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Data Exploration/Pre Cleaning Feature Creation"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import pandas as pd\n",
    "\n",
    "train = pd.read_csv(\"train.csv\")\n",
    "test = pd.read_csv(\"test.csv\")\n",
    "\n",
    "# counting number of words\n",
    "\n",
    "train['word_count_basic'] = train['text'].apply(lambda x: len(str(x).split(\" \")))\n",
    "test['word_count_basic'] = test['text'].apply(lambda x: len(str(x).split(\" \")))\n",
    "\n",
    "# counting number of characters\n",
    "\n",
    "train['char_count_basic'] = train['text'].str.len() ## this also includes spaces\n",
    "test['char_count_basic'] = test['text'].str.len() ## this also includes spaces\n",
    "\n",
    "from nltk.corpus import stopwords\n",
    "\n",
    "stop = stopwords.words('english')\n",
    "\n",
    "train['stopwords_basic'] = train['text'].apply(lambda x: len([x for x in x.split() if x in stop]))\n",
    "test['stopwords_basic'] = test['text'].apply(lambda x: len([x for x in x.split() if x in stop]))\n",
    "\n",
    "\n",
    "# counting number count\n",
    "\n",
    "train['number_count_basic'] = train['text'].apply(lambda x: len([x for x in x.split() if x.isdigit()]))\n",
    "test['number_count_basic'] = test['text'].apply(lambda x: len([x for x in x.split() if x.isdigit()]))\n",
    "\n",
    "# counting special characters\n",
    "\n",
    "def countSpecialChar(str): \n",
    "  \n",
    "    # initializing variables (initial count)\n",
    "    char = 0\n",
    "    specialChar = 0\n",
    "    digit = 0\n",
    "    space = 0\n",
    "  \n",
    "    # str.length() function to count  \n",
    "    # number of character in given string. \n",
    "    for i in range(0, len(str)):  \n",
    "          \n",
    "        ch = str[i]\n",
    "          \n",
    "        if ( (ch >= 'a' and ch <= 'z') or \n",
    "             (ch >= 'A' and ch <= 'Z') ):  \n",
    "            char += 1\n",
    "            \n",
    "        elif (ch >= '0' and ch <= '9'): \n",
    "            digit += 1\n",
    "        elif (ch == ' '):\n",
    "            space += 1\n",
    "        else: \n",
    "            specialChar += 1\n",
    "    return (specialChar)\n",
    "    \n",
    "train['special_character_basic'] = train['text'].apply(countSpecialChar)\n",
    "test['special_character_basic'] = test['text'].apply(countSpecialChar)\n",
    "\n",
    "# counting upper case letters\n",
    "\n",
    "train['upper_count_basic'] = train['text'].apply(lambda x: len([x for x in x.split() if x.isupper()]))\n",
    "test['upper_count_basic'] = test['text'].apply(lambda x: len([x for x in x.split() if x.isupper()]))\n",
    "\n",
    "\n",
    "# counting misspelt words\n",
    "\n",
    "train['text_new'] = train['text'].str.replace('[^\\w\\s]','')\n",
    "train['text_new'] = train['text_new'].apply(lambda x: x.strip())\n",
    "train['words'] = train['text_new'].str.split(\" \")\n",
    "\n",
    "from spellchecker import SpellChecker\n",
    "\n",
    "spell = SpellChecker()\n",
    "\n",
    "train['misspelt_count_basic'] = train['words'].apply(lambda x: len([i for i in x if spell.unknown(i)]))\n",
    "\n",
    "train[['text','misspelt_count_basic']].head()\n",
    "\n",
    "test['text_new'] = test['text'].str.replace('[^\\w\\s]','')\n",
    "test['text_new'] = test['text_new'].apply(lambda x: x.strip())\n",
    "test['words'] = test['text_new'].str.split(\" \")\n",
    "\n",
    "from spellchecker import SpellChecker\n",
    "\n",
    "spell = SpellChecker()\n",
    "\n",
    "test['misspelt_count_basic'] = test['words'].apply(lambda x: len([i for i in x if spell.unknown(i)]))\n",
    "\n",
    "# counting proportion stopword, special, number, upper, misspelt, word length\n",
    "\n",
    "train['stopword_ratio'] = train.stopwords_basic/train.word_count_basic\n",
    "train['special_ratio'] = train.special_character_basic/train.word_count_basic\n",
    "train['number_ratio'] = train.number_count_basic/train.word_count_basic\n",
    "train['upper_ratio'] = train.upper_count_basic/train.word_count_basic\n",
    "train['misspelt_ratio'] = train.misspelt_count_basic/train.word_count_basic\n",
    "train['avg_word_basic'] = (train.char_count_basic-train.word_count_basic+1)/train.word_count_basic\n",
    "\n",
    "train = train.drop(['char_count_basic', 'stopwords_basic', 'number_count_basic', 'upper_count_basic' , 'text_new', 'words'], axis=1)\n",
    "\n",
    "test['stopword_ratio'] = test.stopwords_basic/test.word_count_basic\n",
    "test['special_ratio'] = test.special_character_basic/test.word_count_basic\n",
    "test['number_ratio'] = test.number_count_basic/test.word_count_basic\n",
    "test['upper_ratio'] = test.misspelt_count_basic/test.word_count_basic\n",
    "test['misspelt_ratio'] = train.misspelt_count_basic/train.word_count_basic\n",
    "\n",
    "test = test.drop(['char_count_basic', 'stopwords_basic', 'number_count_basic', 'upper_count_basic', 'text_new', 'words'], axis=1)\n",
    "\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Data Cleaning - Train Set"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [],
   "source": [
    "import pandas as pd\n",
    "from nltk.corpus import stopwords\n",
    "from textblob import TextBlob\n",
    "from textblob import Word\n",
    "import matplotlib.pyplot as plt\n",
    "import numpy as np\n",
    "import sklearn\n",
    "from sklearn.linear_model import LinearRegression"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "train = pd.read_csv(\"train.csv\")\n",
    "test = pd.read_csv(\"test.csv\")\n",
    "\n",
    "#Get initial look at data\n",
    "train.head()\n",
    "\n",
    "#lowercase\n",
    "train['text'] = train['text'].apply(lambda x: \" \".join(x.lower() for x in x.split()))\n",
    "train['text'].head()\n",
    "\n",
    "#remove punctuation\n",
    "train['text'] = train['text'].str.replace('[^\\w\\s]','')\n",
    "train['word_count2'] = train['text'].apply(lambda x: len(str(x).split(\" \")))\n",
    "train['text'].head()\n",
    "\n",
    "#remove stop words\n",
    "stop = stopwords.words('english')\n",
    "train['text'] = train['text'].apply(lambda x: \" \".join(x for x in x.split() if x not in stop))\n",
    "train['word_count3'] = train['text'].apply(lambda x: len(str(x).split(\" \")))\n",
    "train['text'].head()\n",
    "\n",
    "freq = pd.Series(' '.join(train['text']).split()).value_counts()[-10:]\n",
    "\n",
    "#remove uncommon words\n",
    "freq = list(freq.index)\n",
    "train['text'] = train['text'].apply(lambda x: \" \".join(x for x in x.split() if x not in freq))\n",
    "train['text'].head()\n",
    "\n",
    "###remove common words###\n",
    "freq = pd.Series(' '.join(train['text']).split()).value_counts()[:10]\n",
    "freq = list(freq.index)\n",
    "train['text'] = train['text'].apply(lambda x: \" \".join(x for x in x.split() if x not in freq))\n",
    "#Visualize\n",
    "train['text'].head()\n",
    "\n",
    "###Remove rare words###\n",
    "freq = pd.Series(' '.join(train['text']).split()).value_counts()[-10:]\n",
    "freq = list(freq.index)\n",
    "train['text'] = train['text'].apply(lambda x: \" \".join(x for x in x.split() if x not in freq))\n",
    "#Visualize\n",
    "train['text'].head()\n",
    "\n",
    "###Spelling correction###\n",
    "\n",
    "train['text'][:5].apply(lambda x: str(TextBlob(x).correct()))\n",
    "\n",
    "###Tokenization (dividing the text into a sequence of words or sentences)###\n",
    "TextBlob(train['text'][1]).words\n",
    "\n",
    "#Lemmatization (more effective option than stemming because \n",
    "# converts the word into its root word, rather than just stripping the suffices)\n",
    "#*Preferred to stemming*#\n",
    "\n",
    "train['text'] = train['text'].apply(lambda x: \" \".join([Word(word).lemmatize() for word in x.split()]))\n",
    "train['text'].head()\n",
    "\n",
    "#add sentiment analysis\n",
    "sentiment = train['text'].apply(lambda x: TextBlob(x).sentiment[0])\n",
    "\n",
    "train['sentiment'] = sentiment\n",
    "\n",
    "train.to_csv('cleanedtraintext.csv')"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Data Cleaning - Test Set"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#lowercase\n",
    "test['text'] = test['text'].apply(lambda x: \" \".join(x.lower() for x in x.split()))\n",
    "test['text'].head()\n",
    "\n",
    "#remove punctuation\n",
    "test['text'] = test['text'].str.replace('[^\\w\\s]','')\n",
    "test['word_count2'] = test['text'].apply(lambda x: len(str(x).split(\" \")))\n",
    "test['text'].head()\n",
    "\n",
    "#remove stop words\n",
    "from nltk.corpus import stopwords\n",
    "stop = stopwords.words('english')\n",
    "test['text'] = test['text'].apply(lambda x: \" \".join(x for x in x.split() if x not in stop))\n",
    "test['word_count3'] = test['text'].apply(lambda x: len(str(x).split(\" \")))\n",
    "test['text'].head()\n",
    "\n",
    "freq = pd.Series(' '.join(test['text']).split()).value_counts()[-10:]\n",
    "\n",
    "#remove uncommon words\n",
    "freq = list(freq.index)\n",
    "test['text'] = test['text'].apply(lambda x: \" \".join(x for x in x.split() if x not in freq))\n",
    "\n",
    "###remove common words###\n",
    "freq = pd.Series(' '.join(test['text']).split()).value_counts()[:10]\n",
    "freq = list(freq.index)\n",
    "test['text'] = test['text'].apply(lambda x: \" \".join(x for x in x.split() if x not in freq))\n",
    "\n",
    "###Remove rare words###\n",
    "freq = pd.Series(' '.join(test['text']).split()).value_counts()[-10:]\n",
    "freq = list(freq.index)\n",
    "test['text'] = test['text'].apply(lambda x: \" \".join(x for x in x.split() if x not in freq))\n",
    "\n",
    "###Spelling correction###\n",
    "from textblob import TextBlob\n",
    "test['text'][:5].apply(lambda x: str(TextBlob(x).correct()))\n",
    "\n",
    "###Tokenization (dividing the text into a sequence of words or sentences)###\n",
    "TextBlob(test['text'][1]).words\n",
    "\n",
    "#Lemmatization (more effective option than stemming because \n",
    "# converts the word into its root word, rather than just stripping the suffices)\n",
    "#*Preferred to stemming*#\n",
    "\n",
    "from textblob import Word\n",
    "test['text'] = test['text'].apply(lambda x: \" \".join([Word(word).lemmatize() for word in x.split()]))\n",
    "test['text'].head()\n",
    "\n",
    "#add sentiment analysis\n",
    "sentiment = test['text'].apply(lambda x: TextBlob(x).sentiment[0])\n",
    "test['sentiment'] = sentiment\n",
    "\n",
    "test.to_csv('cleanedtesttext.csv')"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "LDA"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import pandas as pd\n",
    "from nltk.corpus import stopwords\n",
    "from textblob import TextBlob\n",
    "from textblob import Word\n",
    "import matplotlib.pyplot as plt\n",
    "import numpy as np\n",
    "import sklearn\n",
    "from sklearn.linear_model import LinearRegression\n",
    "from gensim import corpora, models, similarities\n",
    "import os\n",
    "import tempfile\n",
    "TEMP_FOLDER = tempfile.gettempdir()\n",
    "\n",
    "#Train/test split for validation\n",
    "trainval = train.sample(88692)\n",
    "train = train.loc[~train.index.isin(trainval.index), :]\n",
    "\n",
    "#split apart response\n",
    "trainy = train['age']\n",
    "valy = trainval['age']\n",
    "\n",
    "#prepare train data\n",
    "traintext = train[\"text\"]\n",
    "traintext = traintext.values.tolist()\n",
    "traintext = [str(word) for word in traintext]\n",
    "texts = [[word for word in document.lower().split() ]\n",
    "         for document in traintext]\n",
    "\n",
    "#create dictionary\n",
    "dictionary = corpora.Dictionary(texts)\n",
    "dictionary.filter_extremes(no_below=5, no_above=0.5, keep_n=100000)\n",
    "\n",
    "#save file\n",
    "dictionary.save(os.path.join(TEMP_FOLDER, 'deerwester.dict'))\n",
    "corpus = [dictionary.doc2bow(text) for text in texts]\n",
    "corpora.MmCorpus.serialize(os.path.join(TEMP_FOLDER, 'deerwester.mm'), corpus)  # store to disk, for later use\n",
    "from gensim.models.ldamulticore import LdaMulticore\n",
    "lda = LdaMulticore(corpus, id2word=dictionary, num_topics=25)\n",
    "\n",
    "#generate topics from model\n",
    "import gensim\n",
    "trainlda = lda[corpus]\n",
    "topicstrain = lda.get_document_topics(corpus, minimum_probability=0.0)\n",
    "all_topics_csrtrain = gensim.matutils.corpus2csc(topicstrain)\n",
    "all_topics_numpytrain = all_topics_csrtrain.T.toarray()\n",
    "\n",
    "#validation text\n",
    "new_doc = trainval['text']\n",
    "valtext = new_doc.values.tolist()\n",
    "valtext = [str(word) for word in valtext]\n",
    "valtext = [[word for word in document.lower().split() ]\n",
    "         for document in valtext]\n",
    "valcorpus = [dictionary.doc2bow(text) for text in valtext]\n",
    "\n",
    "#generate topics on validation\n",
    "trainlda = lda[valcorpus]\n",
    "topicsval = lda.get_document_topics(valcorpus, minimum_probability=0.0)\n",
    "all_topics_csrval = gensim.matutils.corpus2csc(topicsval)\n",
    "all_topics_numpyval = all_topics_csrval.T.toarray()\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "TFIDF"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import pandas as pd\n",
    "from nltk.corpus import stopwords\n",
    "from textblob import TextBlob\n",
    "from textblob import Word\n",
    "import matplotlib.pyplot as plt\n",
    "import numpy as np\n",
    "import sklearn\n",
    "\n",
    "from sklearn.linear_model import LinearRegression\n",
    "from gensim import corpora, models, similarities\n",
    "import os\n",
    "import tempfile\n",
    "TEMP_FOLDER = tempfile.gettempdir()\n",
    "\n",
    "train = pd.read_csv('cleanedtraintext.csv')\n",
    "traintext = train[\"text\"]\n",
    "traintext = traintext.values.tolist()\n",
    "traintext = [str(word) for word in traintext]\n",
    "texts = [[word for word in document.lower().split() ]\n",
    "         for document in traintext]\n",
    "#create dictionary\n",
    "dictionary = corpora.Dictionary(texts)\n",
    "\n",
    "#create model\n",
    "tfidf = models.TfidfModel(corpus) # step 1 -- initialize a model\n",
    "corpus_tfidf = tfidf[corpus]\n",
    "\n",
    "#now tfidf for test set\n",
    "test = pd.read_csv('cleanedtesttext.csv')\n",
    "new_doc = test['text']\n",
    "testtext = new_doc.values.tolist()\n",
    "testtext = [str(word) for word in testtext]\n",
    "testtext = [[word for word in document.lower().split() ]\n",
    "         for document in testtext]\n",
    "testcorpus = [dictionary.doc2bow(text) for text in testtext]\n",
    "testcorpus_tfidf = tfidf[testcorpus]\n",
    "#results to numpy array\n",
    "all_topics_csr = gensim.matutils.corpus2csc(corpus_tfidf)\n",
    "all_topics_numpy = all_topics_csr.T.toarray()\n",
    "all_topics_csr2 = gensim.matutils.corpus2csc(testcorpus_tfidf)\n",
    "all_topics_numpytest = all_topics_csr2.T.toarray()"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.6.5"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
