{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [],
   "source": [
    "import pandas as pd\n",
    "from nltk.corpus import stopwords\n",
    "from textblob import TextBlob\n",
    "from textblob import Word\n",
    "import matplotlib.pyplot as plt\n",
    "import numpy as np\n",
    "import sklearn\n",
    "from sklearn.linear_model import LinearRegression\n",
    "from gensim import corpora, models, similarities"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [],
   "source": [
    "import os\n",
    "import tempfile\n",
    "TEMP_FOLDER = tempfile.gettempdir()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [],
   "source": [
    "train = pd.read_csv('cleanedtraintext.csv')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [],
   "source": [
    "traintext = train[\"text\"]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [],
   "source": [
    "traintext = traintext.values.tolist()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [],
   "source": [
    "traintext = [str(word) for word in traintext]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [],
   "source": [
    "texts = [[word for word in document.lower().split() ]\n",
    "         for document in traintext]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 40,
   "metadata": {},
   "outputs": [],
   "source": [
    "#using smaller dictionary here, as makes lda faster to train\n",
    "dictionary = corpora.Dictionary(texts)\n",
    "dictionary.filter_extremes(no_below=5, no_above=0.5, keep_n=100000)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 41,
   "metadata": {},
   "outputs": [],
   "source": [
    "#need to figure out how to save this\n",
    "dictionary.save(os.path.join(TEMP_FOLDER, 'deerwester.dict'))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "new_doc = \"Human computer interaction\"\n",
    "new_vec = dictionary.doc2bow(new_doc.lower().split())\n",
    "print(new_vec)  # the word \"interaction\" does not appear in the dictionary and is ignored"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {},
   "outputs": [],
   "source": [
    "corpus = [dictionary.doc2bow(text) for text in texts]\n",
    "corpora.MmCorpus.serialize(os.path.join(TEMP_FOLDER, 'deerwester.mm'), corpus)  # store to disk, for later use"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Used files generated from first tutorial\n"
     ]
    }
   ],
   "source": [
    "from gensim import corpora, models, similarities\n",
    "if os.path.isfile(os.path.join(TEMP_FOLDER, 'deerwester.dict')):\n",
    "    dictionary = corpora.Dictionary.load(os.path.join(TEMP_FOLDER, 'deerwester.dict'))\n",
    "    corpus = corpora.MmCorpus(os.path.join(TEMP_FOLDER, 'deerwester.mm'))\n",
    "    print(\"Used files generated from first tutorial\")\n",
    "else:\n",
    "    print(\"Please run first tutorial to generate data set\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "100\n",
      "45\n",
      "file\n"
     ]
    }
   ],
   "source": [
    "print(dictionary[0])\n",
    "print(dictionary[1])\n",
    "print(dictionary[2])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "metadata": {},
   "outputs": [],
   "source": [
    "from gensim.models.ldamulticore import LdaMulticore\n",
    "lda = LdaMulticore(corpus, id2word=dictionary, num_topics=50)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "[(41,\n",
       "  '0.078*\"pm\" + 0.036*\"quotejill\" + 0.028*\"quotejoel\" + 0.015*\"jumper991\" + 0.015*\"da\" + 0.008*\"die\" + 0.007*\"4866380\" + 0.005*\"ich\" + 0.005*\"au\" + 0.005*\"nay\"'),\n",
       " (24,\n",
       "  '0.015*\"well\" + 0.015*\"got\" + 0.009*\"gonna\" + 0.008*\"back\" + 0.008*\"yeah\" + 0.008*\"lol\" + 0.008*\"didnt\" + 0.008*\"oh\" + 0.006*\"today\" + 0.006*\"good\"'),\n",
       " (38,\n",
       "  '0.048*\"love\" + 0.008*\"day\" + 0.007*\"went\" + 0.007*\"would\" + 0.007*\"told\" + 0.007*\"said\" + 0.006*\"much\" + 0.006*\"back\" + 0.005*\"night\" + 0.005*\"want\"'),\n",
       " (39,\n",
       "  '0.027*\"_\" + 0.017*\"sbristowsd6\" + 0.014*\"dan\" + 0.012*\"squid3188\" + 0.011*\"mysteriousbob87\" + 0.009*\"donnie\" + 0.007*\"tofig36\" + 0.007*\"di\" + 0.007*\"yang\" + 0.005*\"smarterchild\"'),\n",
       " (3,\n",
       "  '0.015*\"book\" + 0.007*\"page\" + 0.006*\"read\" + 0.006*\"site\" + 0.005*\"use\" + 0.005*\"also\" + 0.005*\"web\" + 0.005*\"new\" + 0.004*\"work\" + 0.004*\"pm\"'),\n",
       " (10,\n",
       "  '0.014*\"life\" + 0.014*\"thing\" + 0.010*\"people\" + 0.010*\"feel\" + 0.009*\"would\" + 0.009*\"think\" + 0.009*\"want\" + 0.007*\"make\" + 0.007*\"way\" + 0.007*\"friend\"'),\n",
       " (27,\n",
       "  '0.013*\"movie\" + 0.007*\"said\" + 0.006*\"harry\" + 0.006*\"story\" + 0.005*\"book\" + 0.005*\"film\" + 0.005*\"police\" + 0.005*\"first\" + 0.004*\"potter\" + 0.004*\"michael\"'),\n",
       " (45,\n",
       "  '0.007*\"year\" + 0.006*\"2004\" + 0.004*\"city\" + 0.004*\"new\" + 0.004*\"said\" + 0.004*\"would\" + 0.003*\"team\" + 0.003*\"first\" + 0.003*\"july\" + 0.003*\"company\"'),\n",
       " (29,\n",
       "  '0.015*\"day\" + 0.011*\"work\" + 0.010*\"today\" + 0.010*\"going\" + 0.010*\"well\" + 0.009*\"got\" + 0.009*\"week\" + 0.008*\"good\" + 0.007*\"school\" + 0.007*\"back\"'),\n",
       " (11,\n",
       "  '0.005*\"would\" + 0.004*\"nigger\" + 0.003*\"barbie\" + 0.003*\"day\" + 0.003*\"spam\" + 0.003*\"wa\" + 0.003*\"turtle\" + 0.003*\"ga\" + 0.003*\"people\" + 0.003*\"see\"'),\n",
       " (15,\n",
       "  '0.040*\"ha\" + 0.010*\"boo\" + 0.005*\"hat\" + 0.005*\"good\" + 0.004*\"think\" + 0.004*\"quotejill\" + 0.004*\"yes\" + 0.003*\"illinois\" + 0.003*\"see\" + 0.003*\"143\"'),\n",
       " (18,\n",
       "  '0.010*\"movie\" + 0.008*\"friend\" + 0.005*\"thing\" + 0.004*\"would\" + 0.004*\"class\" + 0.004*\"good\" + 0.004*\"day\" + 0.004*\"think\" + 0.004*\"first\" + 0.003*\"didnt\"'),\n",
       " (6,\n",
       "  '0.011*\"well\" + 0.010*\"people\" + 0.006*\"day\" + 0.005*\"going\" + 0.005*\"night\" + 0.005*\"want\" + 0.005*\"see\" + 0.005*\"thing\" + 0.005*\"think\" + 0.005*\"would\"'),\n",
       " (28,\n",
       "  '0.010*\"fucking\" + 0.008*\"think\" + 0.007*\"hate\" + 0.007*\"would\" + 0.007*\"mother\" + 0.007*\"fuck\" + 0.006*\"cant\" + 0.005*\"make\" + 0.005*\"people\" + 0.005*\"child\"'),\n",
       " (25,\n",
       "  '0.042*\"1\" + 0.041*\"2\" + 0.030*\"3\" + 0.023*\"4\" + 0.021*\"5\" + 0.012*\"6\" + 0.010*\"7\" + 0.010*\"8\" + 0.010*\"10\" + 0.008*\"12\"'),\n",
       " (7,\n",
       "  '0.045*\"u\" + 0.032*\"n\" + 0.015*\"2\" + 0.011*\"e\" + 0.008*\"b\" + 0.008*\"ur\" + 0.008*\"r\" + 0.008*\"den\" + 0.008*\"dun\" + 0.006*\"went\"'),\n",
       " (35,\n",
       "  '0.033*\"blog\" + 0.018*\"post\" + 0.012*\"email\" + 0.012*\"site\" + 0.011*\"x\" + 0.010*\"comment\" + 0.010*\"link\" + 0.010*\"new\" + 0.008*\"blogger\" + 0.007*\"picture\"'),\n",
       " (49,\n",
       "  '0.005*\"new\" + 0.004*\"see\" + 0.004*\"think\" + 0.004*\"band\" + 0.004*\"would\" + 0.004*\"yeah\" + 0.003*\"much\" + 0.003*\"going\" + 0.003*\"got\" + 0.003*\"little\"'),\n",
       " (40,\n",
       "  '0.019*\"went\" + 0.011*\"back\" + 0.008*\"got\" + 0.007*\"around\" + 0.007*\"day\" + 0.006*\"thing\" + 0.005*\"took\" + 0.005*\"home\" + 0.005*\"way\" + 0.004*\"good\"'),\n",
       " (26,\n",
       "  '0.019*\"dream\" + 0.008*\"angel\" + 0.007*\"see\" + 0.007*\"fire\" + 0.006*\"life\" + 0.006*\"come\" + 0.006*\"eye\" + 0.005*\"make\" + 0.005*\"would\" + 0.004*\"scott\"'),\n",
       " (20,\n",
       "  '0.009*\"people\" + 0.004*\"would\" + 0.004*\"government\" + 0.004*\"state\" + 0.004*\"american\" + 0.004*\"also\" + 0.004*\"new\" + 0.004*\"said\" + 0.003*\"world\" + 0.003*\"many\"'),\n",
       " (12,\n",
       "  '0.009*\"law\" + 0.005*\"would\" + 0.005*\"system\" + 0.004*\"computer\" + 0.004*\"need\" + 0.004*\"program\" + 0.004*\"company\" + 0.004*\"make\" + 0.003*\"microsoft\" + 0.003*\"may\"'),\n",
       " (46,\n",
       "  '0.009*\"people\" + 0.009*\"thing\" + 0.005*\"much\" + 0.004*\"space\" + 0.004*\"change\" + 0.004*\"think\" + 0.004*\"new\" + 0.003*\"would\" + 0.003*\"computer\" + 0.003*\"look\"'),\n",
       " (4,\n",
       "  '0.009*\"think\" + 0.008*\"school\" + 0.008*\"thats\" + 0.008*\"well\" + 0.007*\"ive\" + 0.007*\"people\" + 0.006*\"write\" + 0.006*\"writing\" + 0.006*\"going\" + 0.006*\"thing\"'),\n",
       " (47,\n",
       "  '0.018*\"blah\" + 0.017*\"book\" + 0.013*\"read\" + 0.011*\"thing\" + 0.010*\"think\" + 0.009*\"well\" + 0.007*\"reading\" + 0.006*\"something\" + 0.006*\"going\" + 0.006*\"pretty\"'),\n",
       " (23,\n",
       "  '0.009*\"bush\" + 0.008*\"people\" + 0.007*\"would\" + 0.007*\"war\" + 0.006*\"u\" + 0.005*\"iraq\" + 0.005*\"president\" + 0.004*\"country\" + 0.004*\"think\" + 0.004*\"say\"'),\n",
       " (37,\n",
       "  '0.008*\"night\" + 0.006*\"back\" + 0.004*\"good\" + 0.004*\"day\" + 0.004*\"got\" + 0.004*\"little\" + 0.004*\"would\" + 0.004*\"didnt\" + 0.004*\"people\" + 0.004*\"room\"'),\n",
       " (16,\n",
       "  '0.006*\"weight\" + 0.006*\"year\" + 0.005*\"food\" + 0.004*\"day\" + 0.004*\"would\" + 0.004*\"work\" + 0.004*\"life\" + 0.004*\"health\" + 0.003*\"even\" + 0.003*\"diet\"'),\n",
       " (2,\n",
       "  '0.010*\"woman\" + 0.007*\"sex\" + 0.007*\"phone\" + 0.006*\"men\" + 0.006*\"art\" + 0.004*\"show\" + 0.004*\"good\" + 0.004*\"artist\" + 0.003*\"want\" + 0.003*\"something\"'),\n",
       " (34,\n",
       "  '0.009*\"day\" + 0.008*\"u\" + 0.007*\"think\" + 0.006*\"went\" + 0.005*\"got\" + 0.005*\"back\" + 0.004*\"see\" + 0.004*\"would\" + 0.004*\"much\" + 0.004*\"well\"'),\n",
       " (48,\n",
       "  '0.024*\"game\" + 0.012*\"play\" + 0.008*\"good\" + 0.007*\"going\" + 0.007*\"playing\" + 0.006*\"team\" + 0.006*\"well\" + 0.006*\"think\" + 0.006*\"player\" + 0.005*\"last\"'),\n",
       " (43,\n",
       "  '0.028*\"song\" + 0.014*\"music\" + 0.009*\"band\" + 0.008*\"album\" + 0.008*\"cd\" + 0.007*\"good\" + 0.007*\"new\" + 0.006*\"ive\" + 0.006*\"work\" + 0.005*\"think\"'),\n",
       " (19,\n",
       "  '0.014*\"well\" + 0.009*\"day\" + 0.009*\"good\" + 0.009*\"think\" + 0.008*\"today\" + 0.008*\"ill\" + 0.007*\"much\" + 0.007*\"going\" + 0.007*\"feel\" + 0.006*\"got\"'),\n",
       " (36,\n",
       "  '0.029*\"na\" + 0.022*\"la\" + 0.021*\"de\" + 0.018*\"sa\" + 0.018*\"ko\" + 0.013*\"ng\" + 0.013*\"ang\" + 0.012*\"que\" + 0.010*\"pa\" + 0.009*\"ako\"'),\n",
       " (14,\n",
       "  '0.024*\"dog\" + 0.019*\"cat\" + 0.007*\"oh\" + 0.006*\"much\" + 0.006*\"little\" + 0.005*\"animal\" + 0.005*\"blog\" + 0.005*\"fish\" + 0.005*\"meow\" + 0.004*\"pet\"'),\n",
       " (1,\n",
       "  '0.007*\"car\" + 0.006*\"back\" + 0.005*\"see\" + 0.005*\"would\" + 0.005*\"around\" + 0.004*\"could\" + 0.004*\"road\" + 0.003*\"way\" + 0.003*\"light\" + 0.003*\"eye\"'),\n",
       " (33,\n",
       "  '0.006*\"chicken\" + 0.006*\"well\" + 0.006*\"fun\" + 0.006*\"back\" + 0.005*\"good\" + 0.005*\"went\" + 0.005*\"would\" + 0.004*\"got\" + 0.004*\"new\" + 0.004*\"home\"'),\n",
       " (0,\n",
       "  '0.007*\"well\" + 0.007*\"good\" + 0.007*\"day\" + 0.006*\"dave\" + 0.005*\"think\" + 0.005*\"jason\" + 0.005*\"night\" + 0.005*\"would\" + 0.004*\"monkey\" + 0.004*\"say\"'),\n",
       " (22,\n",
       "  '0.027*\"youre\" + 0.019*\"brought\" + 0.013*\"quizilla\" + 0.013*\"quiz\" + 0.008*\"yes\" + 0.006*\"think\" + 0.006*\"well\" + 0.005*\"take\" + 0.004*\"people\" + 0.004*\"please\"'),\n",
       " (44,\n",
       "  '0.011*\"went\" + 0.011*\"haha\" + 0.010*\"today\" + 0.009*\"got\" + 0.008*\"de\" + 0.007*\"la\" + 0.005*\"well\" + 0.005*\"still\" + 0.005*\"day\" + 0.005*\"le\"')]"
      ]
     },
     "execution_count": 16,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "lda.print_topics(40)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#now tfidf for test set\n",
    "test = pd.read_csv('cleanedtesttext.csv')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "test.head()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "new_doc = test['text']\n",
    "testtext = new_doc.values.tolist()\n",
    "testtext = [str(word) for word in testtext]\n",
    "testtext = [[word for word in document.lower().split() ]\n",
    "         for document in testtext]\n",
    "testtext[:5]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "testcorpus = [dictionary.doc2bow(text) for text in testtext]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.6.5"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
