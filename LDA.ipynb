{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [],
   "source": [
    "import pandas as pd\n",
    "from nltk.corpus import stopwords\n",
    "from textblob import TextBlob\n",
    "from textblob import Word\n",
    "import matplotlib.pyplot as plt\n",
    "import numpy as np\n",
    "import sklearn\n",
    "from sklearn.linear_model import LinearRegression\n",
    "from gensim import corpora, models, similarities"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [],
   "source": [
    "import os\n",
    "import tempfile\n",
    "TEMP_FOLDER = tempfile.gettempdir()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [],
   "source": [
    "train = pd.read_csv('cleanedtraintext.csv')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [],
   "source": [
    "traintext = train[\"text\"]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [],
   "source": [
    "traintext = traintext.values.tolist()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [],
   "source": [
    "traintext = [str(word) for word in traintext]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [],
   "source": [
    "texts = [[word for word in document.lower().split() ]\n",
    "         for document in traintext]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 40,
   "metadata": {},
   "outputs": [],
   "source": [
    "#using smaller dictionary here, as makes lda faster to train\n",
    "dictionary = corpora.Dictionary(texts)\n",
    "dictionary.filter_extremes(no_below=5, no_above=0.5, keep_n=100000)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 41,
   "metadata": {},
   "outputs": [],
   "source": [
    "#need to figure out how to save this\n",
    "dictionary.save(os.path.join(TEMP_FOLDER, 'deerwester.dict'))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "new_doc = \"Human computer interaction\"\n",
    "new_vec = dictionary.doc2bow(new_doc.lower().split())\n",
    "print(new_vec)  # the word \"interaction\" does not appear in the dictionary and is ignored"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {},
   "outputs": [],
   "source": [
    "corpus = [dictionary.doc2bow(text) for text in texts]\n",
    "corpora.MmCorpus.serialize(os.path.join(TEMP_FOLDER, 'deerwester.mm'), corpus)  # store to disk, for later use"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Used files generated from first tutorial\n"
     ]
    }
   ],
   "source": [
    "from gensim import corpora, models, similarities\n",
    "if os.path.isfile(os.path.join(TEMP_FOLDER, 'deerwester.dict')):\n",
    "    dictionary = corpora.Dictionary.load(os.path.join(TEMP_FOLDER, 'deerwester.dict'))\n",
    "    corpus = corpora.MmCorpus(os.path.join(TEMP_FOLDER, 'deerwester.mm'))\n",
    "    print(\"Used files generated from first tutorial\")\n",
    "else:\n",
    "    print(\"Please run first tutorial to generate data set\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "100\n",
      "45\n",
      "file\n"
     ]
    }
   ],
   "source": [
    "print(dictionary[0])\n",
    "print(dictionary[1])\n",
    "print(dictionary[2])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "metadata": {},
   "outputs": [],
   "source": [
    "from gensim.models.ldamulticore import LdaMulticore\n",
    "lda = LdaMulticore(corpus, id2word=dictionary, num_topics=50)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "[(41,\n",
       "  '0.078*\"pm\" + 0.036*\"quotejill\" + 0.028*\"quotejoel\" + 0.015*\"jumper991\" + 0.015*\"da\" + 0.008*\"die\" + 0.007*\"4866380\" + 0.005*\"ich\" + 0.005*\"au\" + 0.005*\"nay\"'),\n",
       " (24,\n",
       "  '0.015*\"well\" + 0.015*\"got\" + 0.009*\"gonna\" + 0.008*\"back\" + 0.008*\"yeah\" + 0.008*\"lol\" + 0.008*\"didnt\" + 0.008*\"oh\" + 0.006*\"today\" + 0.006*\"good\"'),\n",
       " (38,\n",
       "  '0.048*\"love\" + 0.008*\"day\" + 0.007*\"went\" + 0.007*\"would\" + 0.007*\"told\" + 0.007*\"said\" + 0.006*\"much\" + 0.006*\"back\" + 0.005*\"night\" + 0.005*\"want\"'),\n",
       " (39,\n",
       "  '0.027*\"_\" + 0.017*\"sbristowsd6\" + 0.014*\"dan\" + 0.012*\"squid3188\" + 0.011*\"mysteriousbob87\" + 0.009*\"donnie\" + 0.007*\"tofig36\" + 0.007*\"di\" + 0.007*\"yang\" + 0.005*\"smarterchild\"'),\n",
       " (3,\n",
       "  '0.015*\"book\" + 0.007*\"page\" + 0.006*\"read\" + 0.006*\"site\" + 0.005*\"use\" + 0.005*\"also\" + 0.005*\"web\" + 0.005*\"new\" + 0.004*\"work\" + 0.004*\"pm\"'),\n",
       " (10,\n",
       "  '0.014*\"life\" + 0.014*\"thing\" + 0.010*\"people\" + 0.010*\"feel\" + 0.009*\"would\" + 0.009*\"think\" + 0.009*\"want\" + 0.007*\"make\" + 0.007*\"way\" + 0.007*\"friend\"'),\n",
       " (27,\n",
       "  '0.013*\"movie\" + 0.007*\"said\" + 0.006*\"harry\" + 0.006*\"story\" + 0.005*\"book\" + 0.005*\"film\" + 0.005*\"police\" + 0.005*\"first\" + 0.004*\"potter\" + 0.004*\"michael\"'),\n",
       " (45,\n",
       "  '0.007*\"year\" + 0.006*\"2004\" + 0.004*\"city\" + 0.004*\"new\" + 0.004*\"said\" + 0.004*\"would\" + 0.003*\"team\" + 0.003*\"first\" + 0.003*\"july\" + 0.003*\"company\"'),\n",
       " (29,\n",
       "  '0.015*\"day\" + 0.011*\"work\" + 0.010*\"today\" + 0.010*\"going\" + 0.010*\"well\" + 0.009*\"got\" + 0.009*\"week\" + 0.008*\"good\" + 0.007*\"school\" + 0.007*\"back\"'),\n",
       " (11,\n",
       "  '0.005*\"would\" + 0.004*\"nigger\" + 0.003*\"barbie\" + 0.003*\"day\" + 0.003*\"spam\" + 0.003*\"wa\" + 0.003*\"turtle\" + 0.003*\"ga\" + 0.003*\"people\" + 0.003*\"see\"'),\n",
       " (15,\n",
       "  '0.040*\"ha\" + 0.010*\"boo\" + 0.005*\"hat\" + 0.005*\"good\" + 0.004*\"think\" + 0.004*\"quotejill\" + 0.004*\"yes\" + 0.003*\"illinois\" + 0.003*\"see\" + 0.003*\"143\"'),\n",
       " (18,\n",
       "  '0.010*\"movie\" + 0.008*\"friend\" + 0.005*\"thing\" + 0.004*\"would\" + 0.004*\"class\" + 0.004*\"good\" + 0.004*\"day\" + 0.004*\"think\" + 0.004*\"first\" + 0.003*\"didnt\"'),\n",
       " (6,\n",
       "  '0.011*\"well\" + 0.010*\"people\" + 0.006*\"day\" + 0.005*\"going\" + 0.005*\"night\" + 0.005*\"want\" + 0.005*\"see\" + 0.005*\"thing\" + 0.005*\"think\" + 0.005*\"would\"'),\n",
       " (28,\n",
       "  '0.010*\"fucking\" + 0.008*\"think\" + 0.007*\"hate\" + 0.007*\"would\" + 0.007*\"mother\" + 0.007*\"fuck\" + 0.006*\"cant\" + 0.005*\"make\" + 0.005*\"people\" + 0.005*\"child\"'),\n",
       " (25,\n",
       "  '0.042*\"1\" + 0.041*\"2\" + 0.030*\"3\" + 0.023*\"4\" + 0.021*\"5\" + 0.012*\"6\" + 0.010*\"7\" + 0.010*\"8\" + 0.010*\"10\" + 0.008*\"12\"'),\n",
       " (7,\n",
       "  '0.045*\"u\" + 0.032*\"n\" + 0.015*\"2\" + 0.011*\"e\" + 0.008*\"b\" + 0.008*\"ur\" + 0.008*\"r\" + 0.008*\"den\" + 0.008*\"dun\" + 0.006*\"went\"'),\n",
       " (35,\n",
       "  '0.033*\"blog\" + 0.018*\"post\" + 0.012*\"email\" + 0.012*\"site\" + 0.011*\"x\" + 0.010*\"comment\" + 0.010*\"link\" + 0.010*\"new\" + 0.008*\"blogger\" + 0.007*\"picture\"'),\n",
       " (49,\n",
       "  '0.005*\"new\" + 0.004*\"see\" + 0.004*\"think\" + 0.004*\"band\" + 0.004*\"would\" + 0.004*\"yeah\" + 0.003*\"much\" + 0.003*\"going\" + 0.003*\"got\" + 0.003*\"little\"'),\n",
       " (40,\n",
       "  '0.019*\"went\" + 0.011*\"back\" + 0.008*\"got\" + 0.007*\"around\" + 0.007*\"day\" + 0.006*\"thing\" + 0.005*\"took\" + 0.005*\"home\" + 0.005*\"way\" + 0.004*\"good\"'),\n",
       " (26,\n",
       "  '0.019*\"dream\" + 0.008*\"angel\" + 0.007*\"see\" + 0.007*\"fire\" + 0.006*\"life\" + 0.006*\"come\" + 0.006*\"eye\" + 0.005*\"make\" + 0.005*\"would\" + 0.004*\"scott\"'),\n",
       " (20,\n",
       "  '0.009*\"people\" + 0.004*\"would\" + 0.004*\"government\" + 0.004*\"state\" + 0.004*\"american\" + 0.004*\"also\" + 0.004*\"new\" + 0.004*\"said\" + 0.003*\"world\" + 0.003*\"many\"'),\n",
       " (12,\n",
       "  '0.009*\"law\" + 0.005*\"would\" + 0.005*\"system\" + 0.004*\"computer\" + 0.004*\"need\" + 0.004*\"program\" + 0.004*\"company\" + 0.004*\"make\" + 0.003*\"microsoft\" + 0.003*\"may\"'),\n",
       " (46,\n",
       "  '0.009*\"people\" + 0.009*\"thing\" + 0.005*\"much\" + 0.004*\"space\" + 0.004*\"change\" + 0.004*\"think\" + 0.004*\"new\" + 0.003*\"would\" + 0.003*\"computer\" + 0.003*\"look\"'),\n",
       " (4,\n",
       "  '0.009*\"think\" + 0.008*\"school\" + 0.008*\"thats\" + 0.008*\"well\" + 0.007*\"ive\" + 0.007*\"people\" + 0.006*\"write\" + 0.006*\"writing\" + 0.006*\"going\" + 0.006*\"thing\"'),\n",
       " (47,\n",
       "  '0.018*\"blah\" + 0.017*\"book\" + 0.013*\"read\" + 0.011*\"thing\" + 0.010*\"think\" + 0.009*\"well\" + 0.007*\"reading\" + 0.006*\"something\" + 0.006*\"going\" + 0.006*\"pretty\"'),\n",
       " (23,\n",
       "  '0.009*\"bush\" + 0.008*\"people\" + 0.007*\"would\" + 0.007*\"war\" + 0.006*\"u\" + 0.005*\"iraq\" + 0.005*\"president\" + 0.004*\"country\" + 0.004*\"think\" + 0.004*\"say\"'),\n",
       " (37,\n",
       "  '0.008*\"night\" + 0.006*\"back\" + 0.004*\"good\" + 0.004*\"day\" + 0.004*\"got\" + 0.004*\"little\" + 0.004*\"would\" + 0.004*\"didnt\" + 0.004*\"people\" + 0.004*\"room\"'),\n",
       " (16,\n",
       "  '0.006*\"weight\" + 0.006*\"year\" + 0.005*\"food\" + 0.004*\"day\" + 0.004*\"would\" + 0.004*\"work\" + 0.004*\"life\" + 0.004*\"health\" + 0.003*\"even\" + 0.003*\"diet\"'),\n",
       " (2,\n",
       "  '0.010*\"woman\" + 0.007*\"sex\" + 0.007*\"phone\" + 0.006*\"men\" + 0.006*\"art\" + 0.004*\"show\" + 0.004*\"good\" + 0.004*\"artist\" + 0.003*\"want\" + 0.003*\"something\"'),\n",
       " (34,\n",
       "  '0.009*\"day\" + 0.008*\"u\" + 0.007*\"think\" + 0.006*\"went\" + 0.005*\"got\" + 0.005*\"back\" + 0.004*\"see\" + 0.004*\"would\" + 0.004*\"much\" + 0.004*\"well\"'),\n",
       " (48,\n",
       "  '0.024*\"game\" + 0.012*\"play\" + 0.008*\"good\" + 0.007*\"going\" + 0.007*\"playing\" + 0.006*\"team\" + 0.006*\"well\" + 0.006*\"think\" + 0.006*\"player\" + 0.005*\"last\"'),\n",
       " (43,\n",
       "  '0.028*\"song\" + 0.014*\"music\" + 0.009*\"band\" + 0.008*\"album\" + 0.008*\"cd\" + 0.007*\"good\" + 0.007*\"new\" + 0.006*\"ive\" + 0.006*\"work\" + 0.005*\"think\"'),\n",
       " (19,\n",
       "  '0.014*\"well\" + 0.009*\"day\" + 0.009*\"good\" + 0.009*\"think\" + 0.008*\"today\" + 0.008*\"ill\" + 0.007*\"much\" + 0.007*\"going\" + 0.007*\"feel\" + 0.006*\"got\"'),\n",
       " (36,\n",
       "  '0.029*\"na\" + 0.022*\"la\" + 0.021*\"de\" + 0.018*\"sa\" + 0.018*\"ko\" + 0.013*\"ng\" + 0.013*\"ang\" + 0.012*\"que\" + 0.010*\"pa\" + 0.009*\"ako\"'),\n",
       " (14,\n",
       "  '0.024*\"dog\" + 0.019*\"cat\" + 0.007*\"oh\" + 0.006*\"much\" + 0.006*\"little\" + 0.005*\"animal\" + 0.005*\"blog\" + 0.005*\"fish\" + 0.005*\"meow\" + 0.004*\"pet\"'),\n",
       " (1,\n",
       "  '0.007*\"car\" + 0.006*\"back\" + 0.005*\"see\" + 0.005*\"would\" + 0.005*\"around\" + 0.004*\"could\" + 0.004*\"road\" + 0.003*\"way\" + 0.003*\"light\" + 0.003*\"eye\"'),\n",
       " (33,\n",
       "  '0.006*\"chicken\" + 0.006*\"well\" + 0.006*\"fun\" + 0.006*\"back\" + 0.005*\"good\" + 0.005*\"went\" + 0.005*\"would\" + 0.004*\"got\" + 0.004*\"new\" + 0.004*\"home\"'),\n",
       " (0,\n",
       "  '0.007*\"well\" + 0.007*\"good\" + 0.007*\"day\" + 0.006*\"dave\" + 0.005*\"think\" + 0.005*\"jason\" + 0.005*\"night\" + 0.005*\"would\" + 0.004*\"monkey\" + 0.004*\"say\"'),\n",
       " (22,\n",
       "  '0.027*\"youre\" + 0.019*\"brought\" + 0.013*\"quizilla\" + 0.013*\"quiz\" + 0.008*\"yes\" + 0.006*\"think\" + 0.006*\"well\" + 0.005*\"take\" + 0.004*\"people\" + 0.004*\"please\"'),\n",
       " (44,\n",
       "  '0.011*\"went\" + 0.011*\"haha\" + 0.010*\"today\" + 0.009*\"got\" + 0.008*\"de\" + 0.007*\"la\" + 0.005*\"well\" + 0.005*\"still\" + 0.005*\"day\" + 0.005*\"le\"')]"
      ]
     },
     "execution_count": 16,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "lda.print_topics(40)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": 17,
   "metadata": {},
   "outputs": [],
   "source": [
    "#now lda for test set\n",
    "test = pd.read_csv('cleanedtesttext.csv')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 18,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>Unnamed: 0</th>\n",
       "      <th>post.id</th>\n",
       "      <th>user.id</th>\n",
       "      <th>gender</th>\n",
       "      <th>topic</th>\n",
       "      <th>sign</th>\n",
       "      <th>date</th>\n",
       "      <th>text</th>\n",
       "      <th>word_count2</th>\n",
       "      <th>word_count3</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>0</td>\n",
       "      <td>96</td>\n",
       "      <td>4876</td>\n",
       "      <td>female</td>\n",
       "      <td>indUnk</td>\n",
       "      <td>Capricorn</td>\n",
       "      <td>08,August,2004</td>\n",
       "      <td>new tell god create link column</td>\n",
       "      <td>16</td>\n",
       "      <td>8</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1</th>\n",
       "      <td>1</td>\n",
       "      <td>97</td>\n",
       "      <td>4876</td>\n",
       "      <td>female</td>\n",
       "      <td>indUnk</td>\n",
       "      <td>Capricorn</td>\n",
       "      <td>08,August,2004</td>\n",
       "      <td>election rolled around everyone spitting venom...</td>\n",
       "      <td>783</td>\n",
       "      <td>405</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2</th>\n",
       "      <td>2</td>\n",
       "      <td>108</td>\n",
       "      <td>12227</td>\n",
       "      <td>female</td>\n",
       "      <td>Student</td>\n",
       "      <td>Aries</td>\n",
       "      <td>31,July,2004</td>\n",
       "      <td>met lim morning went spc help raymondmathsno c...</td>\n",
       "      <td>116</td>\n",
       "      <td>72</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>3</th>\n",
       "      <td>3</td>\n",
       "      <td>109</td>\n",
       "      <td>12227</td>\n",
       "      <td>female</td>\n",
       "      <td>Student</td>\n",
       "      <td>Aries</td>\n",
       "      <td>30,July,2004</td>\n",
       "      <td>boring day comment found shortest girl class h...</td>\n",
       "      <td>41</td>\n",
       "      <td>17</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>4</th>\n",
       "      <td>4</td>\n",
       "      <td>110</td>\n",
       "      <td>12227</td>\n",
       "      <td>female</td>\n",
       "      <td>Student</td>\n",
       "      <td>Aries</td>\n",
       "      <td>29,July,2004</td>\n",
       "      <td>nut usual theo played food today creation cons...</td>\n",
       "      <td>261</td>\n",
       "      <td>154</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "</div>"
      ],
      "text/plain": [
       "   Unnamed: 0  post.id  user.id  gender    topic       sign            date  \\\n",
       "0           0       96     4876  female   indUnk  Capricorn  08,August,2004   \n",
       "1           1       97     4876  female   indUnk  Capricorn  08,August,2004   \n",
       "2           2      108    12227  female  Student      Aries    31,July,2004   \n",
       "3           3      109    12227  female  Student      Aries    30,July,2004   \n",
       "4           4      110    12227  female  Student      Aries    29,July,2004   \n",
       "\n",
       "                                                text  word_count2  word_count3  \n",
       "0                    new tell god create link column           16            8  \n",
       "1  election rolled around everyone spitting venom...          783          405  \n",
       "2  met lim morning went spc help raymondmathsno c...          116           72  \n",
       "3  boring day comment found shortest girl class h...           41           17  \n",
       "4  nut usual theo played food today creation cons...          261          154  "
      ]
     },
     "execution_count": 18,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "test.head()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 19,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "[['new', 'tell', 'god', 'create', 'link', 'column'],\n",
       " ['election',\n",
       "  'rolled',\n",
       "  'around',\n",
       "  'everyone',\n",
       "  'spitting',\n",
       "  'venom',\n",
       "  'hope',\n",
       "  'impressing',\n",
       "  'public',\n",
       "  'tact',\n",
       "  'umwhat',\n",
       "  'instead',\n",
       "  'pointing',\n",
       "  'greedy',\n",
       "  'little',\n",
       "  'finger',\n",
       "  'accusing',\n",
       "  'mr',\n",
       "  'better',\n",
       "  'tell',\n",
       "  'u',\n",
       "  'youre',\n",
       "  'going',\n",
       "  'u',\n",
       "  'sick',\n",
       "  'hearing',\n",
       "  'badly',\n",
       "  'didnt',\n",
       "  'say',\n",
       "  'guess',\n",
       "  'isnt',\n",
       "  'anyone',\n",
       "  'bound',\n",
       "  'believe',\n",
       "  'anything',\n",
       "  'white',\n",
       "  'collared',\n",
       "  'putz',\n",
       "  'say',\n",
       "  'anywya',\n",
       "  'always',\n",
       "  'opposite',\n",
       "  'ive',\n",
       "  'trying',\n",
       "  'educate',\n",
       "  'different',\n",
       "  'political',\n",
       "  'platform',\n",
       "  'figure',\n",
       "  'identify',\n",
       "  'personally',\n",
       "  'think',\n",
       "  'lying',\n",
       "  'would',\n",
       "  'gather',\n",
       "  'vote',\n",
       "  'shit',\n",
       "  'wonder',\n",
       "  'libertarian',\n",
       "  'never',\n",
       "  'make',\n",
       "  'office',\n",
       "  'blatantly',\n",
       "  'state',\n",
       "  'eliminate',\n",
       "  'entire',\n",
       "  'social',\n",
       "  'welfare',\n",
       "  'system',\n",
       "  'includes',\n",
       "  'eliminating',\n",
       "  'afdc',\n",
       "  'food',\n",
       "  'stamp',\n",
       "  'subsidized',\n",
       "  'housing',\n",
       "  'rest',\n",
       "  'individual',\n",
       "  'unable',\n",
       "  'fully',\n",
       "  'support',\n",
       "  'family',\n",
       "  'job',\n",
       "  'market',\n",
       "  'must',\n",
       "  'learn',\n",
       "  'rely',\n",
       "  'supportive',\n",
       "  'family',\n",
       "  'church',\n",
       "  'community',\n",
       "  'private',\n",
       "  'charity',\n",
       "  'bridge',\n",
       "  'gap',\n",
       "  'ballsy',\n",
       "  'move',\n",
       "  'oh',\n",
       "  'thought',\n",
       "  'there',\n",
       "  'bulk',\n",
       "  'welfare',\n",
       "  'tax',\n",
       "  'dollar',\n",
       "  'go',\n",
       "  'pay',\n",
       "  'handsome',\n",
       "  'salary',\n",
       "  'welleducated',\n",
       "  'welfare',\n",
       "  'worker',\n",
       "  'poor',\n",
       "  'little',\n",
       "  'government',\n",
       "  'welfare',\n",
       "  'except',\n",
       "  'meager',\n",
       "  'handout',\n",
       "  'cycle',\n",
       "  'despair',\n",
       "  'againummmm',\n",
       "  'let',\n",
       "  'rightgovernment',\n",
       "  'stop',\n",
       "  'paying',\n",
       "  'meager',\n",
       "  'handout',\n",
       "  'family',\n",
       "  'need',\n",
       "  'instead',\n",
       "  'family',\n",
       "  'begging',\n",
       "  'church',\n",
       "  'charity',\n",
       "  'even',\n",
       "  'smaller',\n",
       "  'handoutsbecausethose',\n",
       "  'meager',\n",
       "  'handout',\n",
       "  'causing',\n",
       "  'moneywell',\n",
       "  'dry',\n",
       "  'hey',\n",
       "  'ok',\n",
       "  'throw',\n",
       "  'cash',\n",
       "  'foreign',\n",
       "  'country',\n",
       "  'help',\n",
       "  'people',\n",
       "  'care',\n",
       "  'deteste',\n",
       "  'united',\n",
       "  'state',\n",
       "  'somewhere',\n",
       "  'heart',\n",
       "  'pray',\n",
       "  'annihilation',\n",
       "  'bet',\n",
       "  'little',\n",
       "  'bastard',\n",
       "  'setting',\n",
       "  'flag',\n",
       "  'fire',\n",
       "  'stomping',\n",
       "  '911',\n",
       "  'arent',\n",
       "  'aware',\n",
       "  'money',\n",
       "  'covered',\n",
       "  'birth',\n",
       "  'nothing',\n",
       "  'better',\n",
       "  'paying',\n",
       "  'future',\n",
       "  'terrorism',\n",
       "  'maybe',\n",
       "  'taking',\n",
       "  'personally',\n",
       "  'without',\n",
       "  'ever',\n",
       "  'popular',\n",
       "  'welfare',\n",
       "  'system',\n",
       "  'mother',\n",
       "  'might',\n",
       "  'suffered',\n",
       "  'blah',\n",
       "  'see',\n",
       "  'thinking',\n",
       "  'precious',\n",
       "  'green',\n",
       "  'way',\n",
       "  'helped',\n",
       "  'become',\n",
       "  'fine',\n",
       "  'young',\n",
       "  'woman',\n",
       "  'today',\n",
       "  'bzzzzzzzt',\n",
       "  'think',\n",
       "  'sum',\n",
       "  'monies',\n",
       "  'mother',\n",
       "  'received',\n",
       "  'could',\n",
       "  'accounted',\n",
       "  'tax',\n",
       "  'paid',\n",
       "  'immediate',\n",
       "  'family',\n",
       "  'alone',\n",
       "  'could',\n",
       "  'welfare',\n",
       "  'another',\n",
       "  '20',\n",
       "  'year',\n",
       "  'technically',\n",
       "  'tax',\n",
       "  'paid',\n",
       "  'extended',\n",
       "  'family',\n",
       "  'would',\n",
       "  'covered',\n",
       "  'ill',\n",
       "  'thank',\n",
       "  'give',\n",
       "  'middle',\n",
       "  'finger',\n",
       "  'arrogant',\n",
       "  'enough',\n",
       "  'think',\n",
       "  'impact',\n",
       "  'family',\n",
       "  'financial',\n",
       "  'prosperity',\n",
       "  'shoe',\n",
       "  'foot',\n",
       "  'however',\n",
       "  'go',\n",
       "  'tax',\n",
       "  'paying',\n",
       "  'sistersmothersbestfriendsbrothernieces',\n",
       "  'illegitimate',\n",
       "  'kid',\n",
       "  'ease',\n",
       "  'silent',\n",
       "  'curiousity',\n",
       "  'mother',\n",
       "  'successful',\n",
       "  'businesswoman',\n",
       "  'suck',\n",
       "  'tampon',\n",
       "  'string',\n",
       "  'another',\n",
       "  'liberal',\n",
       "  'issue',\n",
       "  'surely',\n",
       "  'cure',\n",
       "  'nation',\n",
       "  'healtcare',\n",
       "  'war',\n",
       "  'drug',\n",
       "  'let',\n",
       "  'legalise',\n",
       "  'typical',\n",
       "  'liberal',\n",
       "  'party',\n",
       "  'promote',\n",
       "  'legalisation',\n",
       "  'drug',\n",
       "  'isnt',\n",
       "  'mean',\n",
       "  'stance',\n",
       "  'issue',\n",
       "  'alone',\n",
       "  'smoking',\n",
       "  'snorting',\n",
       "  'something',\n",
       "  'liberal',\n",
       "  'take',\n",
       "  'anything',\n",
       "  'seems',\n",
       "  'cant',\n",
       "  'control',\n",
       "  'let',\n",
       "  'legalise',\n",
       "  'save',\n",
       "  'money',\n",
       "  'would',\n",
       "  'take',\n",
       "  'correct',\n",
       "  'see',\n",
       "  'way',\n",
       "  'youre',\n",
       "  'plain',\n",
       "  'idiot',\n",
       "  'rehabiliation',\n",
       "  'cost',\n",
       "  'money',\n",
       "  'damage',\n",
       "  'cost',\n",
       "  'money',\n",
       "  'ok',\n",
       "  'give',\n",
       "  'kid',\n",
       "  'easy',\n",
       "  'access',\n",
       "  'pot',\n",
       "  'fall',\n",
       "  'asleep',\n",
       "  'wheel',\n",
       "  'crash',\n",
       "  'building',\n",
       "  'kill',\n",
       "  'someone',\n",
       "  'drug',\n",
       "  'legal',\n",
       "  'individual',\n",
       "  'right',\n",
       "  'decide',\n",
       "  'put',\n",
       "  'body',\n",
       "  'long',\n",
       "  'take',\n",
       "  'responsibility',\n",
       "  'action',\n",
       "  'ok',\n",
       "  'well',\n",
       "  'user',\n",
       "  'irresponsible',\n",
       "  'action',\n",
       "  'endanger',\n",
       "  'people',\n",
       "  'let',\n",
       "  'guess',\n",
       "  'parent',\n",
       "  'bear',\n",
       "  'responsibility',\n",
       "  'riiight',\n",
       "  'addict',\n",
       "  'control',\n",
       "  'tiny',\n",
       "  'minority',\n",
       "  'umso',\n",
       "  'obesity',\n",
       "  'see',\n",
       "  'thats',\n",
       "  'change',\n",
       "  'food',\n",
       "  'legal',\n",
       "  'almost',\n",
       "  'picture',\n",
       "  'it1972',\n",
       "  'demonstration',\n",
       "  'public',\n",
       "  'sex',\n",
       "  'art',\n",
       "  'smoking',\n",
       "  'pot',\n",
       "  'girlfriend',\n",
       "  'vagina',\n",
       "  'group',\n",
       "  'teenage',\n",
       "  'dropout',\n",
       "  'arrested',\n",
       "  'spent',\n",
       "  'night',\n",
       "  'jail',\n",
       "  'concocted',\n",
       "  'notion',\n",
       "  'everything',\n",
       "  'legalised',\n",
       "  'standard',\n",
       "  'living',\n",
       "  'would',\n",
       "  'improved',\n",
       "  'fuck',\n",
       "  'consequence',\n",
       "  'drug',\n",
       "  'induced',\n",
       "  'people',\n",
       "  'fuck',\n",
       "  'work',\n",
       "  'performance',\n",
       "  'fuck',\n",
       "  'cost',\n",
       "  'consequence',\n",
       "  'fuck',\n",
       "  'everyone',\n",
       "  'else',\n",
       "  'liberal',\n",
       "  'party',\n",
       "  'formed'],\n",
       " ['met',\n",
       "  'lim',\n",
       "  'morning',\n",
       "  'went',\n",
       "  'spc',\n",
       "  'help',\n",
       "  'raymondmathsno',\n",
       "  'choice',\n",
       "  'lacos',\n",
       "  'there',\n",
       "  'math',\n",
       "  'quiz',\n",
       "  'coming',\n",
       "  'lecturer',\n",
       "  'damn',\n",
       "  'boring',\n",
       "  'seriously',\n",
       "  'yesterday',\n",
       "  'first',\n",
       "  'listen',\n",
       "  'pay',\n",
       "  'attention',\n",
       "  'math',\n",
       "  'minimal',\n",
       "  'distraction',\n",
       "  'went',\n",
       "  'tp',\n",
       "  'afternoon',\n",
       "  'janice',\n",
       "  'mandystuded',\n",
       "  'library',\n",
       "  'till',\n",
       "  'closebadminton',\n",
       "  '5pm',\n",
       "  'tp',\n",
       "  'sport',\n",
       "  'halli',\n",
       "  'managed',\n",
       "  'little',\n",
       "  'stuff',\n",
       "  'tp',\n",
       "  'co',\n",
       "  'cant',\n",
       "  'study',\n",
       "  'outsidemany',\n",
       "  'pple',\n",
       "  'plus',\n",
       "  'distractionsso',\n",
       "  'irritating',\n",
       "  'headed',\n",
       "  'tm',\n",
       "  'thatwent',\n",
       "  'ntuc',\n",
       "  'buy',\n",
       "  'fruit',\n",
       "  'bar',\n",
       "  'janice',\n",
       "  'didnt',\n",
       "  'allow',\n",
       "  'home',\n",
       "  'accompanied',\n",
       "  'yamahashe',\n",
       "  'wanna',\n",
       "  'buy',\n",
       "  'new',\n",
       "  'guitar',\n",
       "  'rich',\n",
       "  'girl'],\n",
       " ['boring',\n",
       "  'day',\n",
       "  'comment',\n",
       "  'found',\n",
       "  'shortest',\n",
       "  'girl',\n",
       "  'class',\n",
       "  'healthy',\n",
       "  'food',\n",
       "  'today',\n",
       "  'shall',\n",
       "  'mention',\n",
       "  'co',\n",
       "  'cant',\n",
       "  'bothered',\n",
       "  'right'],\n",
       " ['nut',\n",
       "  'usual',\n",
       "  'theo',\n",
       "  'played',\n",
       "  'food',\n",
       "  'today',\n",
       "  'creation',\n",
       "  'consisted',\n",
       "  'usa',\n",
       "  'waffle',\n",
       "  'ice',\n",
       "  'cream',\n",
       "  'hershey',\n",
       "  'choc',\n",
       "  'syrup',\n",
       "  'qoo',\n",
       "  'lemon',\n",
       "  'ice',\n",
       "  'kachang',\n",
       "  'maple',\n",
       "  'syrup',\n",
       "  'chilli',\n",
       "  'sauce',\n",
       "  'butter',\n",
       "  'tomato',\n",
       "  'ketchup',\n",
       "  'interesting',\n",
       "  'something',\n",
       "  'worse',\n",
       "  'theo',\n",
       "  'poured',\n",
       "  'solution',\n",
       "  'roys',\n",
       "  'drink',\n",
       "  'crap',\n",
       "  'lesson',\n",
       "  'still',\n",
       "  'crapping',\n",
       "  'making',\n",
       "  'fun',\n",
       "  'hofm',\n",
       "  'theo',\n",
       "  'imitated',\n",
       "  'latrying',\n",
       "  'funny',\n",
       "  'tbcm',\n",
       "  'worse',\n",
       "  'half',\n",
       "  'class',\n",
       "  'rest',\n",
       "  'girl',\n",
       "  'paying',\n",
       "  'attention',\n",
       "  'work',\n",
       "  'seriously',\n",
       "  'rest',\n",
       "  'u',\n",
       "  'gave',\n",
       "  'stupid',\n",
       "  'answer',\n",
       "  'theo',\n",
       "  'drawing',\n",
       "  'throughout',\n",
       "  'thing',\n",
       "  'seated',\n",
       "  'right',\n",
       "  'front',\n",
       "  'teacher',\n",
       "  'table',\n",
       "  'first',\n",
       "  'row',\n",
       "  'somemore',\n",
       "  'hahaha',\n",
       "  'sch',\n",
       "  'limwilsontheo',\n",
       "  'went',\n",
       "  'ikea',\n",
       "  'lim',\n",
       "  'wanted',\n",
       "  'buy',\n",
       "  'toy',\n",
       "  'dunno',\n",
       "  'reason',\n",
       "  'went',\n",
       "  'hot',\n",
       "  'dog',\n",
       "  'first',\n",
       "  'cheap',\n",
       "  'nice',\n",
       "  'theo',\n",
       "  'shared',\n",
       "  'co',\n",
       "  'u',\n",
       "  'scared',\n",
       "  'fat',\n",
       "  'played',\n",
       "  'sauce',\n",
       "  'custardchilli',\n",
       "  'tomato',\n",
       "  'mixture',\n",
       "  'niceso',\n",
       "  'smoothhahahawent',\n",
       "  'walk',\n",
       "  'heethen',\n",
       "  'wilson',\n",
       "  'suddenly',\n",
       "  'appeared',\n",
       "  'behind',\n",
       "  'u',\n",
       "  'nvm',\n",
       "  'went',\n",
       "  'tm',\n",
       "  'theo',\n",
       "  'wilsonnbspafter',\n",
       "  'kept',\n",
       "  'laughing',\n",
       "  'mrt',\n",
       "  'theo',\n",
       "  'kept',\n",
       "  'imitaing',\n",
       "  'making',\n",
       "  'fun',\n",
       "  'hofm',\n",
       "  'n',\n",
       "  'geokengwahahawilson',\n",
       "  'went',\n",
       "  'arcade',\n",
       "  'play',\n",
       "  'waiting',\n",
       "  'friend',\n",
       "  'theo',\n",
       "  'went',\n",
       "  'walk',\n",
       "  'went',\n",
       "  'ntuc',\n",
       "  'went',\n",
       "  'home',\n",
       "  'went',\n",
       "  'orchard',\n",
       "  'tml',\n",
       "  'still',\n",
       "  'play',\n",
       "  'food',\n",
       "  'co',\n",
       "  'got',\n",
       "  'half',\n",
       "  'hr',\n",
       "  'break',\n",
       "  'good']]"
      ]
     },
     "execution_count": 19,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "new_doc = test['text']\n",
    "testtext = new_doc.values.tolist()\n",
    "testtext = [str(word) for word in testtext]\n",
    "testtext = [[word for word in document.lower().split() ]\n",
    "         for document in testtext]\n",
    "testtext[:5]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 20,
   "metadata": {},
   "outputs": [],
   "source": [
    "testcorpus = [dictionary.doc2bow(text) for text in testtext]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 21,
   "metadata": {},
   "outputs": [],
   "source": [
    "testlda = lda[testcorpus]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 84,
   "metadata": {},
   "outputs": [],
   "source": [
    "topics = lda.get_document_topics(testcorpus, minimum_probability=0.0)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 85,
   "metadata": {},
   "outputs": [],
   "source": [
    "import gensim\n",
    "all_topics_csr = gensim.matutils.corpus2csc(topics)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 86,
   "metadata": {},
   "outputs": [],
   "source": [
    "all_topics_numpy = all_topics_csr.T.toarray()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 90,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "array([0.00285714, 0.00285714, 0.00285714, 0.00285714, 0.00285714,\n",
       "       0.00285714, 0.00285714, 0.00285714, 0.00285714, 0.00285714,\n",
       "       0.00285714, 0.00285714, 0.00285714, 0.00285714, 0.00285714,\n",
       "       0.00285714, 0.00285714, 0.00285714, 0.00285714, 0.00285714,\n",
       "       0.00285714, 0.00285714, 0.00285714, 0.00285714, 0.00285714,\n",
       "       0.00285714, 0.00285714, 0.00285714, 0.00285714, 0.00285714,\n",
       "       0.00285714, 0.00285714, 0.00285714, 0.00285714, 0.00285714,\n",
       "       0.68025362, 0.00285714, 0.00285714, 0.00285714, 0.00285714,\n",
       "       0.00285714, 0.00285714, 0.18260351, 0.00285714, 0.00285714,\n",
       "       0.00285714, 0.00285714, 0.00285714, 0.00285714, 0.00285714])"
      ]
     },
     "execution_count": 90,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "all_topics_numpy[0]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 91,
   "metadata": {},
   "outputs": [],
   "source": [
    "trainlda = lda[corpus]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 92,
   "metadata": {},
   "outputs": [],
   "source": [
    "topicstrain = lda.get_document_topics(corpus, minimum_probability=0.0)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 93,
   "metadata": {},
   "outputs": [],
   "source": [
    "all_topics_csrtrain = gensim.matutils.corpus2csc(topicstrain)\n",
    "all_topics_numpytrain = all_topics_csrtrain.T.toarray()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 95,
   "metadata": {},
   "outputs": [],
   "source": [
    "np.save('numpyldatest', all_topics_numpy)\n",
    "np.save('numpyldatrain', all_topics_numpytrain)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.6.5"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
